<!DOCTYPE html>
<html lang="en">
  <head>
        <title>Today I Learned - Deliberate Friction - A Data Engineering Case Study</title>
      <meta charset="utf-8" />
      <meta name="generator" content="Pelican" />
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Daily Learning Journal" />




    <meta name="tags" content="data-quality" />
    <meta name="tags" content="architecture" />
    <meta name="tags" content="pubsub" />
    <meta name="tags" content="analytics" />
    <meta name="tags" content="case-study" />

  </head>

  <body>
      <header>
          <hgroup><h1><a href="https://ben-cliff.github.io/">Today I Learned</a></h1><p>Daily Learning Journal</p></hgroup>
          <nav><ul>
                <li><a href="https://ben-cliff.github.io/category/data-engineering.html"  aria-current="page" >Data Engineering</a></li>
                <li><a href="https://ben-cliff.github.io/category/meta.html" >Meta</a></li>
          </ul></nav>
      </header>
      <main>
  <article>
    <header>
      <h2>
        <a href="https://ben-cliff.github.io/posts/deliberate-friction/" rel="bookmark"
           title="Permalink to Deliberate Friction - A Data Engineering Case Study">Deliberate Friction - A Data Engineering Case Study</a></h2>
      
    </header>
    <h1>Deliberate Friction - A Data Engineering Case Study</h1>
<h2>Introduction</h2>
<p>In software engineering, we're taught to eliminate friction. Continuous integration, automated pipelines, and frictionless deployments are cornerstones of modern development practices. But what if some friction is exactly what we need? This case study explores how deliberately introducing friction into data workflows improved data quality, reliability, and ultimately business outcomes.</p>
<h2>The Evolution of Data Engineering</h2>
<p>As the field of data engineering progresses, we're seeing a shift in responsibilities. Data engineers are no longer just pipeline builders—they're becoming bridges between backend development and analytics. With analytics engineers moving closer to business stakeholders, data engineers must leverage their computer science background to understand both developer constraints and the needs of analysts, product managers, and business KPIs.</p>
<p>Our organization, like many others, struggled with a classic problem: disconnection between product vision, technical implementation, and data analysis. Product managers would request tracking without clear specifications, developers would implement events without understanding analytics needs, and data teams would struggle to translate the resulting data into meaningful insights.</p>
<h2>The Problem: Data Without Context</h2>
<p>Our analytics ecosystem wasn't optimized to support the next growth phase, where success hinged on measuring the impact of big bets and new features. We had two primary data sources, each with significant limitations:</p>
<ol>
<li>
<p><strong>Backend tables</strong>: Built by developers to manage technical workings of features, not to reflect how users interact with various components or what constitutes 'success.' These tables provided a real-time snapshot but lacked historical perspective due to data overwriting.</p>
</li>
<li>
<p><strong>Frontend tracking (Mixpanel)</strong>: Suffered from multiple issues including:</p>
</li>
<li>Lack of centralized ownership</li>
<li>Reliability problems (ad blockers causing ~10-12% data loss)</li>
<li>Growing maintenance debt (ballooning event count with little documentation)</li>
<li>Testing complexity within QA processes</li>
</ol>
<p>When asking PMs how they'd measure the success of an upcoming feature, responses varied widely. Similarly, the data team had no consistent method for helping PMs gauge performance. The issue wasn't that the data team wasn't providing data products or that PMs lacked understanding—the problem was with the ecosystem itself.</p>
<h2>Failed Attempts: Creating Soft Friction</h2>
<p>My first attempt to address these issues was through an ADR (Architecture Decision Record) proposing a structured approach to frontend tracking. I defined five roles within an event's lifecycle to establish loose SLAs on a team-by-team basis, from event ideation through to analytics consumption.</p>
<p>I tried implementing a ClickUp board where frontend events would be reviewed by data engineers and analysts before deployment. This created a checkpoint to ensure events made sense from an analytics perspective and addressed PM data literacy gaps.</p>
<p>This approach failed to gain traction. The "soft friction" wasn't enough to change entrenched behaviors, and the lack of technical integration meant it was easily bypassed.</p>
<h2>The Solution: Deliberate Friction Through Event Publishing</h2>
<p>The next approach involved "harder" friction—working directly with developers to implement a publisher-subscriber (PubSub) system for analytics events. This approach embraced the fundamental differences between how data teams and backend teams think, using event-driven architecture to decouple producers and consumers.</p>
<p>The key insight was that data engineers and backend teams often promote competing decisions for their own use cases. Instead of fighting this reality, we embraced it by:</p>
<ol>
<li>Implementing backend event publishing for analytics-oriented data points</li>
<li>Creating a clear decision framework (the Data Decision Map) for when to use each data source</li>
<li>Requiring lightweight ownership from developers without impacting codebase maintainability</li>
</ol>
<p>The PubSub system addressed the historical limitations of our database tables while providing more reliable data than frontend tracking. It created a dedicated channel for analytics data that wasn't subject to the constraints of either previous approach.</p>
<h2>Why Friction Works: Quality Through Checkpoints</h2>
<p>The deliberate friction we introduced served several critical purposes:</p>
<ol>
<li><strong>Data Auditing</strong>: Checkpoints in the process allowed us to verify that events were correctly configured before they reached production</li>
<li><strong>Clear SLAs</strong>: The structured workflow established expectations for each team's responsibilities</li>
<li><strong>Data Trustworthiness</strong>: With proper validation and documentation, the resulting data became more reliable for decision-making</li>
</ol>
<p>Unlike traditional software engineering approaches that aim to minimize friction, we recognized that some friction is beneficial when it comes to data quality. Each checkpoint served as an opportunity to ensure that the data would meet the needs of its ultimate consumers.</p>
<h2>Implementation and Adoption</h2>
<p>We created a clear decision framework to guide teams on when to use each data source:</p>
<ul>
<li><strong>Frontend Data (Mixpanel)</strong>: For understanding product usage and feature interactions within sessions</li>
<li><strong>Backend Tables</strong>: For real-time snapshot of application state</li>
<li><strong>PubSub Events</strong>: For tracking historical movements and transformations not logged in tables</li>
</ul>
<p>This framework helped stakeholders make appropriate choices without requiring deep technical knowledge. It also emphasized the strengths and limitations of each approach, improving data literacy across teams.</p>
<p>The implementation required collaboration across multiple teams:
- Infrastructure changes to support the PubSub architecture
- Developer education on event publishing standards
- Documentation of the event schema and usage guidelines
- Analysts training on working with the new data source</p>
<p><img alt="Data Decision Flowchart" src="https://ben-cliff.github.io/images/data-decision-flow.png"></p>
<p><strong>Figure 1: Data Decision Flowchart</strong>. This decision tree guides teams through selecting the appropriate data source based on their analytical needs. Starting with a data question, it first determines if the analysis is standalone or requires feature comparison. For standalone analysis, it evaluates tolerance for data loss and historical logging requirements. The flowchart directs users to either PubSub events (for critical standalone analysis), Mixpanel (for behavioral insights with acceptable data loss), or Backend Tables (for current state analysis without historical requirements).</p>
<h2>Results: Bridging the Gap</h2>
<p>The implementation of deliberate friction through our PubSub system and decision framework provided several key benefits:</p>
<ol>
<li><strong>Improved Data Quality</strong>: Events were properly configured and validated before reaching analytics</li>
<li><strong>Clear Ownership</strong>: Each step in the process had defined responsibilities</li>
<li><strong>Better Feature Measurement</strong>: PMs gained reliable data for measuring feature performance</li>
<li><strong>Reduced Technical Debt</strong>: Centralized approach reduced the proliferation of ad-hoc tracking solutions</li>
</ol>
<p>Perhaps most importantly, it created a shared understanding between product, engineering, and data teams. The friction points became opportunities for cross-functional alignment rather than bottlenecks.</p>
<h2>Lessons Learned</h2>
<p>Through this journey, several key insights emerged:</p>
<ol>
<li><strong>Not All Friction is Bad</strong>: Carefully designed checkpoints improve data quality while maintaining efficiency</li>
<li><strong>Data Engineers as Bridges</strong>: Position between backend and analytics requires advocating for both technical feasibility and analytical needs</li>
<li><strong>Decision Frameworks Matter</strong>: Clear guidelines reduce confusion and improve consistency</li>
<li><strong>Ownership Must Be Explicit</strong>: Without defined responsibilities, data quality suffers</li>
</ol>
<p>The evolution of data engineering requires us to reconsider our aversion to friction. As we bridge the gap between technical implementation and business value, some deliberate friction ensures that data serves its ultimate purpose: informing better decisions.</p>
<h2>Conclusion</h2>
<p>As data engineering evolves, practitioners should consider where friction adds value rather than assuming it should always be eliminated. By creating structured processes for data production—not just consumption—we can significantly improve the quality and usefulness of our analytics.</p>
<p>The counter-intuitive approach of adding friction paid dividends in our organization. It forced important conversations, improved cross-functional understanding, and ultimately delivered more reliable insights for decision-makers.</p>
<p>In your organization, consider where a lack of friction might be causing data quality issues, and don't be afraid to implement deliberate checkpoints that ensure your data can be trusted to drive business outcomes.</p>
    <footer>
      <p>Published: <time datetime="2024-03-20T00:00:00+00:00">
        Wed 20 March 2024
      </time></p>
        <address>
          By             <a href="https://ben-cliff.github.io/author/your-name.html">Your Name</a>
        </address>
        <p>
          Category: <a href="https://ben-cliff.github.io/category/data-engineering.html">Data Engineering</a>
        </p>
        <p>
          Tags:
            <a href="https://ben-cliff.github.io/tag/data-quality.html">data-quality</a>
            <a href="https://ben-cliff.github.io/tag/architecture.html">architecture</a>
            <a href="https://ben-cliff.github.io/tag/pubsub.html">pubsub</a>
            <a href="https://ben-cliff.github.io/tag/analytics.html">analytics</a>
            <a href="https://ben-cliff.github.io/tag/case-study.html">case-study</a>
        </p>
    </footer>
  </article>
      </main>
      <footer>
          <address>
            Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>,
            which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
          </address>
      </footer>
  </body>
</html>