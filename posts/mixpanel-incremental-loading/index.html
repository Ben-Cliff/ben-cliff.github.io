<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://ben-cliff.github.io/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://ben-cliff.github.io/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Ben Cliff" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="dbt, bigquery, analytics, incremental-loading, Mixpanel, event-tracking, Data Engineering, " />

<meta property="og:title" content="Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events "/>
<meta property="og:url" content="https://ben-cliff.github.io/posts/mixpanel-incremental-loading/" />
<meta property="og:description" content="Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events Situation Our Mixpanel implementation had reached an extreme scale, generating a massive volume of event data that made traditional processing methods completely unsustainable. Several critical challenges had emerged: The dataset had grown to an unmanageable size, with a year of …" />
<meta property="og:site_name" content="Today I Learned" />
<meta property="og:article:author" content="Ben Cliff" />
<meta property="og:article:published_time" content="2024-08-07T00:00:00+01:00" />
<meta name="twitter:title" content="Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events ">
<meta name="twitter:description" content="Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events Situation Our Mixpanel implementation had reached an extreme scale, generating a massive volume of event data that made traditional processing methods completely unsustainable. Several critical challenges had emerged: The dataset had grown to an unmanageable size, with a year of …">

        <title>Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events  · Today I Learned
</title>
        <link rel="shortcut icon" href="https://ben-cliff.github.io/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://ben-cliff.github.io/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://ben-cliff.github.io/theme/images/apple-touch-icon-180x180.png" type="image/png" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://ben-cliff.github.io/"><span class=site-name>Today I Learned</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://ben-cliff.github.io
                                    >Home</a>
                                </li>
                                <li ><a href="https://ben-cliff.github.io/categories.html">Categories</a></li>
                                <li ><a href="https://ben-cliff.github.io/tags.html">Tags</a></li>
                                <li ><a href="https://ben-cliff.github.io/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://ben-cliff.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://ben-cliff.github.io/posts/mixpanel-incremental-loading/">
                Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h1>Mixpanel Incremental Processing: Conquering Extreme Data Volumes and Duplicate Events</h1>
<h2>Situation</h2>
<p>Our Mixpanel implementation had reached an extreme scale, generating a massive volume of event data that made traditional processing methods completely unsustainable. Several critical challenges had emerged:</p>
<ol>
<li><strong>The dataset had grown to an unmanageable size, with a year of historical data exceeding several terabytes</strong></li>
<li><strong>Recreating the entire table daily was causing processing times to exceed 8 hours, making daily refreshes impossible</strong></li>
<li>Mixpanel was consistently sending duplicate events (affecting approximately 0.11% of all events), making incremental processing impossible without deduplication</li>
<li>Historical data needed to be backfilled while ensuring data consistency</li>
<li>Reporting systems required timely data without the latency of full-refresh processing</li>
<li><strong>Computing costs had escalated dramatically, with daily processing consuming over 250 slot-hours</strong></li>
</ol>
<p>These issues were affecting both the reliability of our analytics and the cost-effectiveness of our data infrastructure.</p>
<h2>Task</h2>
<p>I needed to implement a robust incremental processing system for Mixpanel data that would:</p>
<ol>
<li><strong>Handle the extreme data volume by processing only new data instead of the entire dataset</strong></li>
<li><strong>Reliably deduplicate Mixpanel events to make incremental processing possible</strong></li>
<li>Create a reliable historical baseline through proper backfilling of terabytes of historical data</li>
<li>Ensure data consistency and accuracy for reporting despite duplicate events</li>
<li><strong>Dramatically reduce processing time from 8+ hours to under 30 minutes</strong></li>
<li><strong>Reduce computing costs by at least 75% without compromising data quality</strong></li>
<li>Create a sustainable pattern that could scale with future data growth</li>
</ol>
<h2>Action</h2>
<p>I developed a comprehensive solution leveraging DBT's incremental processing and BigQuery's advanced features.</p>
<h3>1. Duplicate Event Detection and Handling for Incremental Processing</h3>
<p>For incremental processing to work correctly, duplicate records must be eliminated. I discovered that Mixpanel was sending duplicate events (approximately 0.11% of all data), which would break the incremental merge operations if not addressed. I implemented a robust deduplication system in the staging layer:</p>
<div class="highlight"><pre><span></span><code><span class="err">{{</span>
<span class="w">  </span><span class="n">config</span><span class="p">(</span>
<span class="w">    </span><span class="n">materialized</span><span class="o">=</span><span class="s1">&#39;table&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="n">partition_by</span><span class="o">=</span><span class="err">{</span>
<span class="w">      </span><span class="ss">&quot;field&quot;</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;partition_date&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="ss">&quot;data_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;date&quot;</span>
<span class="w">    </span><span class="err">}</span>
<span class="w">  </span><span class="p">)</span>
<span class="err">}}</span>

<span class="k">SELECT</span>
<span class="w">  </span><span class="o">*</span><span class="p">,</span>
<span class="w">  </span><span class="n">ROW_NUMBER</span><span class="p">()</span><span class="w"> </span><span class="n">OVER</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="n">PARTITION</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">mp_insert_id</span><span class="p">,</span><span class="w"> </span><span class="k">time</span>
<span class="w">    </span><span class="k">ORDER</span><span class="w"> </span><span class="k">BY</span><span class="w"> </span><span class="n">mp_insert_id</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">row_num</span>
<span class="k">FROM</span><span class="w"> </span><span class="err">{{</span><span class="w"> </span><span class="k">source</span><span class="p">(</span><span class="s1">&#39;mixpanel&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mp_master_event&#39;</span><span class="p">)</span><span class="w"> </span><span class="err">}}</span>
</code></pre></div>

<p>This approach:
- Identifies duplicates using Mixpanel's native <code>mp_insert_id</code> and event timestamp
- Assigns a row number to each event, keeping only the first occurrence (<code>row_num = 1</code>)
- Creates a clean foundation for incremental processing by ensuring unique records
- Preserves rejected duplicates in a separate table for monitoring and analysis</p>
<h3>2. Incremental Loading Implementation</h3>
<p>I implemented an efficient incremental loading pattern in DBT:</p>
<div class="highlight"><pre><span></span><code><span class="err">{{</span>
<span class="w">  </span><span class="n">config</span><span class="p">(</span>
<span class="w">    </span><span class="n">materialized</span><span class="o">=</span><span class="s1">&#39;incremental&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="n">unique_key</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mp_insert_id&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;time&#39;</span><span class="p">],</span>
<span class="w">    </span><span class="n">partition_by</span><span class="o">=</span><span class="err">{</span>
<span class="w">      </span><span class="ss">&quot;field&quot;</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;partition_date&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="ss">&quot;data_type&quot;</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;date&quot;</span><span class="p">,</span>
<span class="w">      </span><span class="ss">&quot;granularity&quot;</span><span class="p">:</span><span class="w"> </span><span class="ss">&quot;day&quot;</span>
<span class="w">    </span><span class="err">}</span><span class="p">,</span>
<span class="w">    </span><span class="n">cluster_by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;customer_id&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mp_insert_id&#39;</span><span class="p">],</span>
<span class="w">    </span><span class="n">incremental_strategy</span><span class="o">=</span><span class="s1">&#39;merge&#39;</span><span class="p">,</span>
<span class="w">    </span><span class="n">merge_update_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">]</span>
<span class="w">  </span><span class="p">)</span>
<span class="err">}}</span>

<span class="k">WITH</span><span class="w"> </span><span class="n">source_data</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="p">(</span>
<span class="w">  </span><span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="err">{{</span><span class="w"> </span><span class="k">ref</span><span class="p">(</span><span class="s1">&#39;stg_mp_dedup_events&#39;</span><span class="p">)</span><span class="w"> </span><span class="err">}}</span>
<span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="n">row_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w">  </span><span class="c1">-- Only take first occurrence of any duplicate</span>
<span class="p">)</span>

<span class="k">SELECT</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="n">source_data</span>
<span class="err">{</span><span class="o">%</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">is_incremental</span><span class="p">()</span><span class="w"> </span><span class="o">%</span><span class="err">}</span>
<span class="w">  </span><span class="k">WHERE</span><span class="w"> </span><span class="n">partition_date</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="n">DATE_SUB</span><span class="p">(</span><span class="k">CURRENT_DATE</span><span class="p">(),</span><span class="w"> </span><span class="nb">INTERVAL</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="k">DAY</span><span class="p">)</span>
<span class="w">  </span><span class="k">AND</span><span class="w"> </span><span class="k">NOT</span><span class="w"> </span><span class="k">EXISTS</span><span class="w"> </span><span class="p">(</span>
<span class="w">    </span><span class="k">SELECT</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">FROM</span><span class="w"> </span><span class="err">{{</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="n">t</span>
<span class="w">    </span><span class="k">WHERE</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="n">mp_insert_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">source_data</span><span class="p">.</span><span class="n">mp_insert_id</span>
<span class="w">    </span><span class="k">AND</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">source_data</span><span class="p">.</span><span class="k">time</span>
<span class="w">  </span><span class="p">)</span>
<span class="err">{</span><span class="o">%</span><span class="w"> </span><span class="n">endif</span><span class="w"> </span><span class="o">%</span><span class="err">}</span>
</code></pre></div>

<p>This pattern:
- Only processes events from the last three days to catch late-arriving data
- Uses a compound key (<code>mp_insert_id</code> and <code>time</code>) to precisely identify duplicate events
- Leverages BigQuery's merge capabilities for efficient updates
- Optimizes query performance with partitioning and clustering</p>
<h3>3. Historical Data Backfill</h3>
<p>For the initial backfill of historical data, I created a specialized Python script that could process the entire dataset chronologically while maintaining incremental processing principles:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">process_partition_date</span><span class="p">(</span><span class="n">partition_date</span><span class="p">):</span>
    <span class="n">partition_date_str</span> <span class="o">=</span> <span class="n">partition_date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="c1"># Construct the query to insert deduplicated data</span>
    <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    INSERT INTO `target_table`</span>
<span class="s2">    SELECT t.*</span>
<span class="s2">    FROM (</span>
<span class="s2">        SELECT</span>
<span class="s2">            *,</span>
<span class="s2">            ROW_NUMBER() OVER (PARTITION BY mp_insert_id, time ORDER BY mp_insert_id) AS rn</span>
<span class="s2">        FROM `source_table`</span>
<span class="s2">        WHERE DATE(_PARTITIONTIME) = &#39;</span><span class="si">{</span><span class="n">partition_date_str</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">    ) t</span>
<span class="s2">    LEFT JOIN `processed_keys_table` pk</span>
<span class="s2">        ON t.mp_insert_id = pk.mp_insert_id AND t.time = pk.time</span>
<span class="s2">    WHERE t.rn = 1 AND pk.mp_insert_id IS NULL</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="c1"># Run the query</span>
    <span class="n">query_job</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">query_job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>

    <span class="c1"># Update processed keys to avoid reprocessing</span>
    <span class="n">update_keys_query</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    INSERT INTO `processed_keys_table` (mp_insert_id, time)</span>
<span class="s2">    SELECT DISTINCT mp_insert_id, time</span>
<span class="s2">    FROM `source_table`</span>
<span class="s2">    WHERE DATE(_PARTITIONTIME) = &#39;</span><span class="si">{</span><span class="n">partition_date_str</span><span class="si">}</span><span class="s2">&#39;</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">update_keys_job</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">update_keys_query</span><span class="p">)</span>
    <span class="n">update_keys_job</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</code></pre></div>

<p>This script:
- Processed one day at a time, allowing for manageable chunks
- Tracked which records had been processed using a separate tracking table
- Applied the same deduplication logic used in the ongoing incremental processing
- Ensured data consistency between historical and new data</p>
<h3>4. Rejected Event Tracking</h3>
<p>To monitor data quality, I implemented a system to track rejected events:</p>
<div class="highlight"><pre><span></span><code><span class="k">CREATE</span><span class="w"> </span><span class="k">OR</span><span class="w"> </span><span class="k">REPLACE</span><span class="w"> </span><span class="k">TABLE</span><span class="w"> </span><span class="o">`</span><span class="n">mp_rejected_events</span><span class="o">`</span><span class="w"> </span><span class="k">AS</span>
<span class="k">SELECT</span>
<span class="w">  </span><span class="n">t</span><span class="p">.</span><span class="o">*</span><span class="p">,</span>
<span class="w">  </span><span class="s1">&#39;Duplicate event&#39;</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">rejection_reason</span><span class="p">,</span>
<span class="w">  </span><span class="k">CURRENT_TIMESTAMP</span><span class="p">()</span><span class="w"> </span><span class="k">AS</span><span class="w"> </span><span class="n">rejection_timestamp</span>
<span class="k">FROM</span><span class="w"> </span><span class="err">{{</span><span class="w"> </span><span class="k">source</span><span class="p">(</span><span class="s1">&#39;mixpanel&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mp_master_event&#39;</span><span class="p">)</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="n">t</span>
<span class="k">INNER</span><span class="w"> </span><span class="k">JOIN</span><span class="w"> </span><span class="err">{{</span><span class="w"> </span><span class="k">ref</span><span class="p">(</span><span class="s1">&#39;stg_mp_dedup_events&#39;</span><span class="p">)</span><span class="w"> </span><span class="err">}}</span><span class="w"> </span><span class="n">d</span>
<span class="w">  </span><span class="k">ON</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="n">mp_insert_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d</span><span class="p">.</span><span class="n">mp_insert_id</span>
<span class="w">  </span><span class="k">AND</span><span class="w"> </span><span class="n">t</span><span class="p">.</span><span class="k">time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d</span><span class="p">.</span><span class="k">time</span>
<span class="k">WHERE</span><span class="w"> </span><span class="n">d</span><span class="p">.</span><span class="n">row_num</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span>
</code></pre></div>

<p>This approach:
- Captured all rejected duplicate events
- Provided visibility into the duplication rate (approximately 0.11%)
- Created an audit trail for data quality monitoring</p>
<h2>Results</h2>
<p>The implementation delivered substantial improvements:</p>
<ol>
<li>
<p><strong>Extreme volume handling</strong>: Successfully processed and maintained multiple terabytes of event data without performance degradation</p>
</li>
<li>
<p><strong>Processing efficiency</strong>: Processing time decreased from over 8 hours to approximately 15 minutes per day by only handling new data</p>
</li>
<li>
<p><strong>Cost reduction</strong>: Computing costs decreased by 92% (from 250+ slot-hours to just 20 slot-hours daily) due to the reduced processing volume</p>
</li>
<li>
<p><strong>Data accuracy</strong>: Eliminated all duplicate events through reliable deduplication, improving the accuracy of downstream analytics</p>
</li>
<li>
<p><strong>Historical data consistency</strong>: Successfully backfilled over a year of historical data while maintaining data integrity</p>
</li>
<li>
<p><strong>Query performance</strong>: Partitioning and clustering reduced query times by 65% for common analytical queries</p>
</li>
<li>
<p><strong>Scalable foundation</strong>: Created a system that can handle 10x growth without significant changes to the architecture</p>
</li>
</ol>
<p>This solution has created a robust, efficient pipeline for Mixpanel data that delivers high-quality data to our analytics systems while minimizing processing costs and time, even as data volumes continue to grow.</p>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2024-08-07T00:00:00+01:00">Wed 07 August 2024</time>
            <h4>Category</h4>
            <a class="category-link" href="https://ben-cliff.github.io/categories.html#data-engineering-ref">Data Engineering</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://ben-cliff.github.io/tags.html#analytics-ref">analytics
                    <span class="superscript">4</span>
</a></li>
                <li><a href="https://ben-cliff.github.io/tags.html#bigquery-ref">bigquery
                    <span class="superscript">5</span>
</a></li>
                <li><a href="https://ben-cliff.github.io/tags.html#dbt-ref">dbt
                    <span class="superscript">3</span>
</a></li>
                <li><a href="https://ben-cliff.github.io/tags.html#event-tracking-ref">event-tracking
                    <span class="superscript">3</span>
</a></li>
                <li><a href="https://ben-cliff.github.io/tags.html#incremental-loading-ref">incremental-loading
                    <span class="superscript">3</span>
</a></li>
                <li><a href="https://ben-cliff.github.io/tags.html#mixpanel-ref">Mixpanel
                    <span class="superscript">2</span>
</a></li>
            </ul>
<h4>Stay in Touch</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/Ben-Cliff" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>
    <div>
        CC-BY-4.0
    </div>

    <div>
        <span class="site-name">Today I Learned</span> - Daily Learning Journal
    </div>



    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://ben-cliff.github.io/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>